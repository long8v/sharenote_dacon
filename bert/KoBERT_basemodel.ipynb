{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet-cu101 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from mxnet-cu101) (1.19.4)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from mxnet-cu101) (0.8.4)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from mxnet-cu101) (2.25.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (2020.6.20)\n",
      "Requirement already satisfied: gluonnlp in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (0.10.0)\n",
      "Requirement already satisfied: pandas in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (1.1.4)\n",
      "Requirement already satisfied: tqdm in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (4.53.0)\n",
      "Requirement already satisfied: cython in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from gluonnlp) (0.29.21)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from gluonnlp) (1.19.4)\n",
      "Requirement already satisfied: packaging in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from gluonnlp) (20.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from pandas) (2020.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from packaging->gluonnlp) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Requirement already satisfied: sentencepiece==0.1.85 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (0.1.85)\n",
      "Requirement already satisfied: transformers==2.1.1 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (2.1.1)\n",
      "Requirement already satisfied: requests in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from transformers==2.1.1) (2.25.0)\n",
      "Requirement already satisfied: sentencepiece in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from transformers==2.1.1) (0.1.85)\n",
      "Requirement already satisfied: boto3 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from transformers==2.1.1) (1.16.23)\n",
      "Requirement already satisfied: sacremoses in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from transformers==2.1.1) (0.0.43)\n",
      "Requirement already satisfied: tqdm in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from transformers==2.1.1) (4.53.0)\n",
      "Requirement already satisfied: regex in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from transformers==2.1.1) (2020.11.13)\n",
      "Requirement already satisfied: numpy in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from transformers==2.1.1) (1.19.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from requests->transformers==2.1.1) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from requests->transformers==2.1.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from requests->transformers==2.1.1) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from requests->transformers==2.1.1) (3.0.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from boto3->transformers==2.1.1) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.23 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from boto3->transformers==2.1.1) (1.19.23)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from boto3->transformers==2.1.1) (0.3.3)\n",
      "Requirement already satisfied: click in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from sacremoses->transformers==2.1.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from sacremoses->transformers==2.1.1) (0.17.0)\n",
      "Requirement already satisfied: six in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from sacremoses->transformers==2.1.1) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.23->boto3->transformers==2.1.1) (2.8.1)\n",
      "Requirement already satisfied: torch==1.3.1 in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages (from torch==1.3.1) (1.19.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet-cu101\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece==0.1.85\n",
    "!pip install transformers==2.1.1\n",
    "!pip install torch==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-td9698la\n",
      "Requirement already satisfied (use --upgrade to upgrade): kobert==0.1.1 from git+https://****@github.com/SKTBrain/KoBERT.git@master in /home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages\n",
      "Building wheels for collected packages: kobert\n",
      "  Building wheel for kobert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobert: filename=kobert-0.1.1-py3-none-any.whl size=12824 sha256=a205918f64cedf2e4a9aaaaac4cbb43867e961850490653a5d9f6562ad5b4f53\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-07kw6jn2/wheels/4b/9b/3f/583189713d96fda0f291ca1f228ab0e0c981641dcdaf7c7cdf\n",
      "Successfully built kobert\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "libcublas.so.10: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8f3e0713b200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkobert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkobert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch_kobert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_pytorch_kobert_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/long36v/lib/python3.6/site-packages/kobert/pytorch_kobert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgluonnlp\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/long36v/lib/python3.6/site-packages/gluonnlp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/long36v/lib/python3.6/site-packages/mxnet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\"\"\"MXNet: a concise, fast and flexible framework for deep learning.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_pinned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/long36v/lib/python3.6/site-packages/mxnet/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassproperty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_metaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_MXClassPropertyMetaClass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_LIB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/long36v/lib/python3.6/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;31m# library instance of mxnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;31m# type definitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/long36v/lib/python3.6/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;34m\"\"\"Load library by searching possible path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlib_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_lib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRTLD_LOCAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;31m# DMatrix functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/long36v/lib/python3.6/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: libcublas.so.10: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
    "!wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"/home/miyoun/sangwon.joo/dacon/KoBERT/dacon_ext_summary/data/dacon_train.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\"/home/miyoun/sangwon.joo/dacon/KoBERT/dacon_ext_summary/data/dacon_test.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "#dataset_submission = nlp.data.TSVDataset(\"/home/miyoun/sangwon.joo/dacon/KoBERT/dacon_ext_summary/data/dacon_submission.txt\", field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_step, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_submission = nlp.data.TSVDataset(\"/home/miyoun/sangwon.joo/dacon/KoBERT/dacon_ext_summary/data/dacon_submission.txt\", field_indices=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission = BERTDataset(dataset_submission, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_dataloader = torch.utils.data.DataLoader(data_submission, batch_size=1, num_workers=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "score_dict ={}\n",
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5\n",
    "\n",
    "\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(submission_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "    \n",
    "    score_dict[dataset_submission[batch_id][1]] = out.cpu().data.numpy()[0][1]\n",
    "    \n",
    "    #if batch_id%100 == 0:\n",
    "    #    print(batch_id/len(dataset_submission)*100)\n",
    "    \"\"\"print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    print(token_ids)\n",
    "    print(label)\n",
    "    print(dataset_submission[batch_id][1])\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = pd.read_csv(\"/home/miyoun/sangwon.joo/dacon/KoBERT/dacon_ext_summary/data/dacon_submission.txt\", sep=\"\\t\", header=None)\n",
    "sub_data[\"score\"] = sub_data[2].map(lambda x: score_dict[str(x)])\n",
    "sub_data[\"sent_num\"] = sub_data[2].map(lambda x: str(x)[:-3])\n",
    "\n",
    "\n",
    "sub_data[\"rank\"] = sub_data.groupby(\"sent_num\")[\"score\"].rank(method=\"min\", ascending=False)\n",
    "\n",
    "result_dict={}\n",
    "\n",
    "sent_list = list(set(sub_data[\"sent_num\"]))\n",
    "\n",
    "for sent in sent_list:\n",
    "    tmp = sub_data[sub_data[\"sent_num\"]==sent]\n",
    "    \n",
    "    answer = tmp[tmp[\"rank\"]==1][1].item() + \"\\n\" + tmp[tmp[\"rank\"]==2][1].item() + \"\\n\" + tmp[tmp[\"rank\"]==3][1].item()\n",
    "    \n",
    "    result_dict[sent] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fi = pd.read_csv(\"/home/miyoun/sangwon.joo/dacon/KoBERT/dacon_ext_summary/data/extractive_sample_submission_v2.csv\")\n",
    "sub_fi[\"summary\"] = sub_fi[\"id\"].map(lambda x: result_dict[str(x)])\n",
    "sub_fi.to_csv(\"/home/miyoun/sangwon.joo/dacon/KoBERT/dacon_ext_summary/data/ext_sub_order.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순서대로 변경 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data[\"imp_sentence\"] = sub_data[\"rank\"].map(lambda x: 1 if x <=3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sent_list:\n",
    "    tmp = sub_data[sub_data[\"sent_num\"]==sent]\n",
    "    \n",
    "    answer = \"\\n\".join(list(tmp[tmp[\"imp_sentence\"]==1][1]))\n",
    "    \n",
    "    result_dict[sent] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fi = pd.read_csv(\"/home/miyoun/sangwon.joo/dacon/KoBERT/dacon_ext_summary/data/extractive_sample_submission_v2.csv\")\n",
    "sub_fi[\"summary\"] = sub_fi[\"id\"].map(lambda x: result_dict[str(x)])\n",
    "sub_fi.to_csv(\"/home/miyoun/sangwon.joo/dacon/KoBERT/dacon_ext_summary/data/ext_sub_nonorder.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "long36v",
   "language": "python",
   "name": "long36v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
