{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT embedding loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_files = sorted(glob('../bert_file/*.pt'))\n",
    "pt_files_index = [int(pt.split('_')[-1].split('.')[0]) for pt in pt_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt_files_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../bert_file/doc_11926.pt\n",
      "../bert_file/doc_17742.pt\n"
     ]
    }
   ],
   "source": [
    "for pt in pt_files:\n",
    "    try:\n",
    "        torch.load(pt)\n",
    "    except:\n",
    "        print(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [torch.load(pt) for pt in pt_files\n",
    "          if pt not in ['../bert_file/doc_11926.pt', '../bert_file/doc_17742.pt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../file/train.jsonl') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = [data[index] for index in pt_files_index] # fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_sample = [torch.Tensor([[1,0] if _ in data['extractive'] else [0,1]\n",
    "                            for _ in range(len(data['article_original']))]) \n",
    "                for idx, data in enumerate(data_sample)\n",
    "               if idx not in [11926, 17742]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train-valid split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_y, test_y = train_test_split(tensors, label_sample, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17194, 17194)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformerDataset(Dataset):\n",
    "    def __init__(self, data_x, data_y, transform_x=None, transform_y=None):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_x = self.data_x[idx]\n",
    "        sample_y = self.data_y[idx]\n",
    "        return sample_x, sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "    return xx_pad, yy_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensors = collate_fn_padd(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = transformerDataset(train_data, train_y)\n",
    "dl = DataLoader(ds, batch_size=bs, collate_fn=pad_collate, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = transformerDataset(test_data, test_y)\n",
    "dl_test = DataLoader(ds_test, batch_size=bs, collate_fn=pad_collate, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 34, 768])\n",
      "torch.Size([128, 34, 2])\n"
     ]
    }
   ],
   "source": [
    "for d in dl:\n",
    "    print(d[0].shape)\n",
    "    print(d[1].shape)\n",
    "    break\n",
    "    if d[0].shape[0] == bs:\n",
    "        pass\n",
    "    else:\n",
    "        print(d[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # positional encoding layer 따로 정의 \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model) # positonal encoding\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # 짝수는 sin 함수 적용\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 홀수는 cos 함수 적용\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ninp, nhead, nhid, n_class, dropout=0.5):\n",
    "        # ntoken : number of sentence\n",
    "        # ninp : the number of expected features in the input\n",
    "        # nhead : the number of heads in multi-head attention model \n",
    "        # nhid : the dimension of the feedforward network model \n",
    "        # nlayers : the number of class\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, 1)\n",
    "        self.ninp = ninp\n",
    "        self.linear = nn.Linear(in_features=ninp, out_features=n_class)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.layernorm = nn.LayerNorm(ninp)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # encoder의 nn.Embedding의 레이어 초기화\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        src = self.layernorm(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.linear(output)\n",
    "#         output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device = 'cuda'\n",
    "model = TransformerModel(ninp=768, nhead=64, nhid=4096, n_class=2, dropout=0.5).to(device)\n",
    "lr = 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_accuracy(model):\n",
    "    common = 0\n",
    "    all_common = 0\n",
    "    for idx, test_batch in enumerate(dl_test):\n",
    "        tensor = test_batch[0].to(device)\n",
    "        label = test_batch[1].to(device)\n",
    "        label = label.view(bs, -1, 1)\n",
    "        output = model(tensor)\n",
    "        argtopk = output.topk(3, axis=1)[1]\n",
    "        output = label.topk(3, axis=1)[1]\n",
    "        output = output.view(bs, -1)\n",
    "        argtopk = argtopk.view(bs, -1)\n",
    "        for opt, lbl in zip(output, argtopk):\n",
    "            common += len(set(opt.tolist()).intersection(lbl.tolist()))\n",
    "            all_common += 3\n",
    "    return common / all_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_accuracy(X,Y):\n",
    "#     max_vals, max_indices = torch.max(X, 1)\n",
    "#     train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "#     return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14787946428571427"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_test_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../file/extractive_test_v2.jsonl'\n",
    "file = pd.read_json(filepath, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['summary'] = file.article_original.apply(lambda e: '\\n'.join(e[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file[['id', 'summary']].to_csv('../file/submit.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuray : 0.6854538690476191\n",
      "test accuray : 0.6891741071428571\n",
      "test accuray : 0.6700148809523809\n",
      "test accuray : 0.6225818452380952\n",
      "test accuray : 0.6750372023809523\n",
      "test accuray : 0.6309523809523809\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "accuracy_list = []\n",
    "best_accuracy = 0.5\n",
    "for epoch in range(300):\n",
    "    i = 0 \n",
    "    for batch in dl:\n",
    "        model.train()\n",
    "        tensor = batch[0].to(device)\n",
    "        label = batch[1].to(device)\n",
    "        try:\n",
    "            label = label.view(bs, -1, 2)\n",
    "            output = model(tensor)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            i += 1\n",
    "            if i % 100 == 0 :\n",
    "                loss_list.append(loss)\n",
    "                model.eval()\n",
    "                accuracy = get_test_accuracy(model)\n",
    "                print('test accuray : {}'.format(accuracy))\n",
    "                accuracy_list.append(accuracy)\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_model = model\n",
    "                    best_accuracy = accuracy\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(tensor.shape, output.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import WarmupLinearSchedule\n",
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "dataset_train = nlp.data.TSVDataset(\"/home/sora/Desktop/mkj/tmp/dacon_train.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\"/home/sora/Desktop/mkj/tmp/dacon_test.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=False)\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n",
    "dtype = torch.float\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        self.num_class = num_classes\n",
    "        self.hidden_size = 128\n",
    "        self.final_lstm = nn.LSTM(input_size = hidden_size, hidden_size = self.hidden_size, num_layers=2, bidirectional=True)  # hidden_size * 2 -> *3\n",
    "        self.logits_fc = nn.Linear(self.hidden_size*2, num_classes)\n",
    "        self.dropout = nn.Dropout(self.dr_rate)\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        all_encoder_layers, pooled = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))        # print(all_encoder_layers.shape)\n",
    "        lstm_output, states = self.final_lstm(all_encoder_layers)\n",
    "        lstm_output = lstm_output[-1]\n",
    "        output = self.logits_fc(lstm_output)\n",
    "        out = self.dropout(output)\n",
    "        return out\n",
    "model = BERTClassifier(bertmodel, dr_rate = 0.5).to(device)\n",
    "test_model = BERTClassifier(bertmodel, dr_rate = 1.0).to(device)\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "print(t_total, warmup_step)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_step, t_total=t_total)\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        if len(label) != batch_size:\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        predictions= model(token_ids, valid_length, segment_ids).squeeze(1)\n",
    "        loss = loss_fn(predictions, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(predictions, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "            torch.save(model.state_dict(), 'model.ckpt')\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    # model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = test_model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "# torch.save(model, '/home/sora/Desktop/mkj/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "long36v",
   "language": "python",
   "name": "long36v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
