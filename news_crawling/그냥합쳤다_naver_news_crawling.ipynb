{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query list :  ['빅데이터', 'AI -조류', '딥러닝', '인공지능', '핀테크', '테크핀', '빅데이터+보험', 'AI+보험', '인공지능+보험', '삼성생명+디지털']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1307/1307 [00:14<00:00, 88.36it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 981.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query list :  ['삼성생명', '생명보험', '생보사', '금융위원회', '실손보험', '손해보험', '손보사', '변액보험', '연금보험', ' 종신보험', '금융+보험', '약관대출', '금융당국+보험', '금감원+보험', '금융감독원+보험', 'GA+보험', '보험업계', '보험+디지털', '보험+핀테크']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:33<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2210/2210 [00:24<00:00, 88.64it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 823.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import WarmupLinearSchedule\n",
    "\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob \n",
    "import datetime\n",
    "from get_news_data import get_news_data\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "#del BERTDataset\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    " \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 10\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5\n",
    "\n",
    "keyword_dict={\n",
    "    \"보험\" : [\"삼성생명\", \"생명보험\", \"생보사\", \"금융위원회\", \"실손보험\", \"손해보험\", \"손보사\", \"변액보험\", \"연금보험\", \" 종신보험\", \"금융+보험\", \"약관대출\", \"금융당국+보험\", \"금감원+보험\", \"금융감독원+보험\", \"GA+보험\", \"보험업계\", \"보험+디지털\", \"보험+핀테크\"],\n",
    "    \"디지털\" : [\"빅데이터\", \"AI -조류\", \"딥러닝\", \"인공지능\", \"핀테크\", \"테크핀\", \"빅데이터+보험\", \"AI+보험\", \"인공지능+보험\", \"삼성생명+디지털\"],\n",
    "}\n",
    "\n",
    "from get_news_data import get_news_data\n",
    "\n",
    "for keyword_group in [\"디지털\", \"보험\"]:\n",
    "\n",
    "    news_dic = get_news_data(keyword_group,keyword_dict[keyword_group])\n",
    "\n",
    "\n",
    "    dataset =[]\n",
    "    index_to_cont = {}\n",
    "    for item in news_dic.items():\n",
    "\n",
    "        for idx, cont in enumerate(item[1][\"content\"]):\n",
    "\n",
    "            cont_index = item[0] + str(idx).zfill(3)\n",
    "\n",
    "            dataset.append([cont, cont_index])\n",
    "            index_to_cont[cont_index]={\"cont\":cont}\n",
    "\n",
    "\n",
    "    _, vocab = get_pytorch_kobert_model()\n",
    "    tokenizer = get_tokenizer()\n",
    "    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "    dataset = BERTDataset(dataset, 0, 1, tok, max_len, True, False)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, num_workers=5, shuffle=False)\n",
    "    \n",
    "    dataset =[]\n",
    "    index_to_cont = {}\n",
    "    for item in news_dic.items():\n",
    "\n",
    "        for idx, cont in enumerate(item[1][\"content\"]):\n",
    "\n",
    "            cont_index = item[0] + str(idx).zfill(3)\n",
    "\n",
    "            dataset.append([cont, cont_index])\n",
    "            index_to_cont[cont_index]={\"cont\":cont}\n",
    "            \n",
    "    class BERTClassifier(nn.Module):\n",
    "        def __init__(self,\n",
    "                     bert,\n",
    "                     hidden_size = 768,\n",
    "                     num_classes=2,\n",
    "                     dr_rate=None,\n",
    "                     params=None):\n",
    "            super(BERTClassifier, self).__init__()\n",
    "            self.bert = bert\n",
    "            self.dr_rate = dr_rate\n",
    "\n",
    "            self.classifiernn.Linear(hidden_size , num_classes)\n",
    "\n",
    "            if dr_rate:\n",
    "                self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "        def gen_attention_mask(self, token_ids, valid_length):\n",
    "            attention_mask = torch.zeros_like(token_ids)\n",
    "            for i, v in enumerate(valid_length):\n",
    "                attention_mask[i][:v] = 1\n",
    "            return attention_mask.float()\n",
    "\n",
    "        def forward(self, token_ids, valid_length, segment_ids):\n",
    "            attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "            _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "\n",
    "            if self.dr_rate:\n",
    "                out = self.dropout(pooler)\n",
    "\n",
    "            return self.classifier(out)\n",
    "    \n",
    "    model_name = \"./model/[SOTA]kobert_downstream_3epoch.sav\"\n",
    "    model = pickle.load(open(model_name, 'rb'))\n",
    "\n",
    "    ##GPU 사용 시\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    result_dict = {}\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        #label = label.long().to(device)\n",
    "        #order_ratios=torch.from_numpy(np.asarray(np.float32(order_ratios))).long().to(device)\n",
    "        score = model(token_ids, valid_length, segment_ids)\n",
    "        score = score.to(\"cpu\")\n",
    "        index_to_cont[label[0]][\"score\"]=score.detach().numpy()[0][1]\n",
    "        \n",
    "        \n",
    "    for item in tqdm(news_dic.items()):\n",
    "        news_index = item[0]\n",
    "        cont_index = [x[0] for x in index_to_cont.items() if x[0][:10]==news_index]\n",
    "        output_sorted = sorted({k: index_to_cont[k] for k in index_to_cont.keys() if k in cont_index}.items(), key=lambda x: x[1][\"score\"], reverse=True)\n",
    "\n",
    "        news_dic[item[0]][\"ext_summary\"] = [int(x[0][-3:]) for x in output_sorted][:3]\n",
    "\n",
    "    keyword_list = []\n",
    "    title_list =[]\n",
    "    summary_list = []\n",
    "    content_list =[]\n",
    "    url_list=[]\n",
    "    press_list=[]\n",
    "    for item in news_dic.items():\n",
    "        cont = news_dic[item[0]][\"content\"]\n",
    "        ext_index = news_dic[item[0]][\"ext_summary\"]\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"query : \", item[1][\"query\"])\n",
    "        print(\"title : \", item[1][\"title\"])\n",
    "        print(\"summary : \\n\", \".\".join([cont[x] for x in ext_index]))\n",
    "        print(\"content : \\n\", \".\".join(item[1][\"content\"]))l\n",
    "        print(\"=\"*50)\n",
    "        \"\"\"\n",
    "        \n",
    "        if item[1][\"url\"] not in url_list:\n",
    "            keyword_list.append(item[1][\"query\"])\n",
    "            title_list.append(item[1][\"title\"])\n",
    "            summary_list.append(\"\\n\".join([cont[x] for x in ext_index]))\n",
    "            content_list.append(\"\\n\".join(item[1][\"content\"]))\n",
    "            url_list.append(item[1][\"url\"])\n",
    "            press_list.append(item[1][\"press\"])\n",
    "\n",
    "\n",
    "    today = datetime.date.today()\n",
    "    today.isoformat()\n",
    "\n",
    "\n",
    "    result=pd.DataFrame()\n",
    "\n",
    "    result[\"keyword\"] = keyword_list\n",
    "    result[\"title\"] = title_list\n",
    "    result[\"summary\"] = summary_list\n",
    "    result[\"content\"] = content_list\n",
    "    result[\"url\"] = url_list\n",
    "    result[\"date\"] = today\n",
    "    result[\"press\"]=press_list\n",
    "\n",
    "    fname_list = glob(\"./result/*\")\n",
    "\n",
    "    saved_url=[]\n",
    "    for fname in fname_list:\n",
    "        tmp = pd.read_csv(fname, sep=\"\\t\")\n",
    "        saved_url += list(tmp[\"url\"])\n",
    "\n",
    "    result = result[~result[\"url\"].isin(saved_url)]\n",
    "\n",
    "    print(len(result))\n",
    "    result.to_csv(\"./\"+today.isoformat()+\"_\"+keyword_group+\".tsv\", sep=\"\\t\", encoding=\"utf8\", index=False)\n",
    "    \n",
    "\n",
    "    result_text=\"이 메시지는 NAVER에서 뉴스를 크롤링한 후 BERT를 사용하여 추출요약한 결과입니다\\n\"\n",
    "\n",
    "    for title, summary, url, press in zip(result[\"title\"], result[\"summary\"], result[\"url\"], result[\"press\"]):\n",
    "        result_text += \"\\n\"\n",
    "        result_text += \"-\"*100\n",
    "        result_text += \"\\n\\n\"\n",
    "        result_text += \"<\"+title.strip()+\"> : \"+ press\n",
    "        result_text += \"\\n\"*2\n",
    "        result_text += summary.strip()\n",
    "        result_text += \"\\n\"*2\n",
    "        result_text += url\n",
    "        result_text += \"\\n\"\n",
    "\n",
    "    smtp = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    smtp.ehlo()      # say Hello\n",
    "    smtp.starttls()  # TLS 사용시 필요\n",
    "    smtp.login('godjsw1@gmail.com', '2wldmsdl1')\n",
    "\n",
    "    today = datetime.date.today()\n",
    "    Subject = '[뉴스요약][{keyword}]'.format(keyword=keyword_group) + today.isoformat().split(\"-\")[0] +\"년 \" +today.isoformat().split(\"-\")[1] +\"월 \" +today.isoformat().split(\"-\")[2] +\"일\"\n",
    "\n",
    "    msg = MIMEText(result_text)\n",
    "    msg['Subject'] = Subject\n",
    "    msg['To'] = 'sangwon.joo@samsung.com'\n",
    "    smtp.sendmail('godjsw1@gmail.com', 'sangwon.joo@samsung.com', msg.as_string())\n",
    "    #smtp.sendmail('godjsw1@gmail.com', 'sangwon.joo@samsung.com', msg.as_string())\n",
    "    #smtp.sendmail('godjsw1@gmail.com', 'peter8.lee@samsung.com', msg.as_string())\n",
    "    #smtp.sendmail('godjsw1@gmail.com', 'rami.kim@samsung.com', msg.as_string())\n",
    "    smtp.sendmail('godjsw1@gmail.com', 'jy927.nam@samsung.com', msg.as_string())\n",
    "    smtp.sendmail('godjsw1@gmail.com', 'g0835.min@samsung.com', msg.as_string())\n",
    "\n",
    "    smtp.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
